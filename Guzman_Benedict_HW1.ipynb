{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning in Medicine\n",
    "### BMSC-GA 4493, BMIN-GA 3007 \n",
    "### Spring 2019\n",
    "### Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you need to write mathematical terms, you can type your answeres in a Markdown Cell via LaTex \n",
    "\n",
    "See: <a href=\"https://stackoverflow.com/questions/13208286/how-to-write-latex-in-ipython-notebook\">here</a> if you have issues with writing equations. To see basic LaTex notation see: <a href=\"https://en.wikibooks.org/wiki/LaTeX/Mathematics\"> here </a>.\n",
    "\n",
    "**Submission instruction**: Upload and Submit your final jupyter notebook file in <a href='http://newclasses.nyu.edu '>newclasses.nyu.edu</a>\n",
    "<br>\n",
    "**Submission deadline: Tuesday Feb 26th 2019 (3pm) **\n",
    "\n",
    "**Questions and Clarifications**: You can reach out to your TAs Chhavi Yadav (chhavi[at]nyu.edu), Ren Yi (ren.yi[at]nyu.edu), and Aakash Kaku (ark576[at]nyu.edu)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Question 1: Take Derivatives!  (Total 25 points)\n",
    "### Take derivatives of function f(x) with respect to x in questions 1.1 to 1.5. (1 point each)\n",
    "### For 1.6 and 1.7 take partial derivatives of f(X, A) with respect to each $a_i$ and $x_i$. (4 points each)\n",
    "### You can use conditions in your response to 1.8, 1.9, and 1.10 if needed. For some special values of x, derivative may not exist, and that's ok. (4 points each)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.  $f(x) = 5(x-4)^3 + x -1 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$f'(x) =  15(x-4)^2 + 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2. $f(x) = e^x + e^{-x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$f'(x) = e^x - e^{-x} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3. $f(x) = \\frac{1}{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " $f'(x) = -\\frac {1}{x^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4. $f(x) = \\frac{1}{1+e^{-x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " $ f'(x) = \\frac{e^{-x}}{({1+e^{-x})^2}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5. $f(x) = log_e(x) + log_e(5x) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " $f'(x) = \\frac {2}{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.6. $f(A,X) = \\sum_{i=1}^5 log_e(a_i * x_i)$ \n",
    "<br>\n",
    "where $A = (a_1, a_2, a_3, a_4, a_5)$ and $X = (x_1, x_2, x_3, x_4, x_5)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.7. $f(A,X) = \\sum_{i=1}^5 log_e(\\frac{1}{1+e^{-a_i*x_i}})$\n",
    "<br>\n",
    "where $A = (a_1, a_2, a_3, a_4, a_5)$ and $X = (x_1, x_2, x_3, x_4, x_5)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.8. $ f(x) =\n",
    "  \\begin{cases}\n",
    "    x & \\text{if $x>0$} \\\\\n",
    "    0 & \\text{otherwise}\n",
    "  \\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.9. $f(x) = | x | $\n",
    "<br>\n",
    "($|x|$ means absolute value of x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " $f'(x) = -\\frac {x}{|x|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.10. $f(x) = max(x,  x^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$ f'(x) =\n",
    "  \\begin{cases}\n",
    "    1 & \\text{if $0<x<1$} \\\\\n",
    "    2x & \\text{$x>1 V x<0$}\\\\\n",
    "    indeterminate & \\text{$otherwise$}\n",
    "  \\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Question 2: Solving Linear Regression via Mean Squared Error (MSE) Optimization Problem (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that you have measured two variables X and Y, for a simple task, and you belive that they might be linearly related to each other. Here, our input X has 2 dimensions, and the output has 1 dimension. We will use super-script to indicate which sample it is, and sub-scipt to indicate which dimension it is. \n",
    "The measurements are as follows:\n",
    "\n",
    "###### (Training data D = {($X^1$, $Y^1$), ($X^2$, $Y^2$), ($X^3$, $Y^3$)})\n",
    "\n",
    "Sample 1: $X^1 = (x_1^1, x_2^1) = (1,1)$,   $Y^1$ = 6\n",
    "\n",
    "Sample 2: $X^2 = (x_1^2, x_2^2) = (2,3)$,   $Y^2$ = 11\n",
    "\n",
    "Sample 3: $X^3 = (x_1^3, x_2^3) = (-1,0)$,   $Y^3$ = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume that the relationship between X and Y is linear, we can write this relationship as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Y = f_{W,B}(X) = WX + B = w_1*x_1 + w_2*x_2 + B$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $W = (w_1, w_2)$ and $B$ are the parameters of the model.\t\n",
    "We are interested in finding best values for W and B.\t\n",
    "We define 'best' in terms of a loss function between $f_{W,b}(X_i)$ and $Y_i$ for each ($X_i$ and $Y_i$) in the training data. \t\n",
    "Since $Y_i$s are real numbers, let's consider Mean Squared Error loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that Mean Squared Error for this function, over training data, and W and B is:\n",
    "\n",
    "$MSELoss(D={(X_1, Y_1), (X_2, Y_2), (X_3, Y_3)}), W, B) = \\frac{1}{3}\\sum_{i=1}^{3} (f_{W,B}(X_i) - Y_i)^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) (9 points) \n",
    "Compute the partial derivative of $MESLoss(D, W, B)$, With respect to W and B.\t\n",
    "Remember that $X_1$, $X_2$, $X_3$, $Y_1$, $Y_2$, and $Y_3$ are constants, and already given to us as training data above.\n",
    "\n",
    "$\\frac{d}{d w_1} MSELoss(D, W, B) = ?$\n",
    "\n",
    "$\\frac{d}{d w_2} MSELoss(D, W, B) = ?$\n",
    "\n",
    "$\\frac{d}{d B} MSELoss(D, W, B) = ?$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\n",
    "$\\frac{d}{d w_1} MSELoss(D, W, B) = \\frac{2}{3} (6w + 2b - 26)$\n",
    "\n",
    "$\\frac{d}{d w_2} MSELoss(D, W, B) = \\frac{2}{3} (10w + 4b - 39)$\n",
    "\n",
    "$\\frac{d}{d B} MSELoss(D, W, B) = \\frac{2}{3} (6w + 6b - 38)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) (3 points) \n",
    "Use matplotlib library and plot $\\frac{d}{d w1} MSELoss(D, W, B)$ for $w_1 = np.arange(0,2,0.1)$, when $w_2$ equals 2, and B equals to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "w1 = np.arange(0, 2, 0.1)\n",
    "dfdw1 = (2/3)*(6*w1 + 2*3 - 26)\n",
    "plt.plot(w1, dfdw1)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3) (3 points) \n",
    "What values of $w_1$, $w_2$ and $B$, make all partial derivatives zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4) (10 points) \n",
    "If you start from an initial point $w_1^0 = 0.1$ , $w_2^0 = 0.1$ and $B^0 = 0.1$, and iteratively update your $w_1$, $w_2$, and B via gradient descent as follows:\n",
    "    \n",
    "$ w_1^{t+1} = w_1^t - 0.01 * \\frac{d}{d w_1} MSELoss(D, W, B) |_{w_1^t,w_2^t,B^t} $\t\n",
    "$ w_2^{t+1} = w_2^t - 0.01 * \\frac{d}{d w_2} MSELoss(D, W, B) |_{w_1^t,w_2^t,B^t} $\t\n",
    "$ B^{t+1} = B^t - 0.01 * \\frac{d}{d B} MSELoss(D, W, B) |_{w_1^t,w_2^t,B^t} $\t\n",
    "(Note: This is gradient descent with a 0.01 learning rate.)\n",
    "\n",
    "What are the values of Ws and B over iterations 0 to 50? (Don't compute by hand! Write a code!)\t\n",
    "Write a python script that computes these values for 50 iterations, i.e. lists of $\\{w_1^0, w^1_1,.., w_1^{50}\\}$, $\\{w_2^0, w_2^1,.., w_2^{50}\\}$, and $\\{B^0, B^1,.., B^{50}\\}$.\t\n",
    "Plot the lists of $w_1$s, $w_2$s and Bs over 50 iterations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = 0.1\n",
    "w2 = 0.1 \n",
    "b = 0.1\n",
    "w1_list = []\n",
    "w2_list = []\n",
    "b_list = []\n",
    "\n",
    "for epoch in range(50):\n",
    "    w1 = w1 - 0.01 * (2/3)*(6*w1 + 2*b - 26)\n",
    "    w2 = w2 - 0.01 * (2/3)*(10*w2 + 4*b - 39)\n",
    "    b = b - 0.01 * (2/3)*(6*w1 + 6*b - 38)\n",
    "    w1_list.append(w1)\n",
    "    w2_list.append(w2)\n",
    "    b_list.append(b)\n",
    "\n",
    "print(w1,w2, b)\n",
    "plt.plot(w1_list);plt.show()\n",
    "plt.plot(w2_list);plt.show()\n",
    "plt.plot(b_list);plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5) (10 points) \n",
    "Now that you learned the math and made the code yourself, we will use pytorch and automatic differentiation, to find optimal W and B!\t\n",
    "Again, consider data to be D = {($X_1$, $Y_1$), ($X_2$, $Y_2$), ($X_3$, $Y_3$)}) = {((1,1), 6), ((2,3),11), ((-1,0),2)}.\n",
    "\n",
    "Some of your steps are here. Fill in the rest and show a plot of the loss function, $w_1$, $w_2$ and B over these 10 epochs. (4 plots total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "\n",
    "D = [((1,1), 6), ((2,3),11), ((-1,0),2)]\n",
    "a = np.random.shuffle(D)\n",
    "print(D)\n",
    "X = [d[0] for d in D]\n",
    "Y = [d[1] for d in D]\n",
    "print('data X is:', X)\n",
    "print('data Y is:', Y)\n",
    "\n",
    "model = torch.nn.Linear(2, 1, bias=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "losslist = []\n",
    "w1list = []\n",
    "w2list = []\n",
    "blist = []\n",
    "\n",
    "# for epoch in range(10):\n",
    "    # Shuffle your training data samples\n",
    "    # Loop over your training data in the new order:\n",
    "        #dont forget to: optimizer.zero_grad()\n",
    "        #prepare your x_input and y_target if needed\n",
    "        #send the data through your model: i.e. pred_i = model(x_input)\n",
    "        #send the prediction through the loss function too: i.e. lossout= loss(pred_i, y_target)\n",
    "        #call backward to back-propagate: i.e. lossout.backward()\n",
    "        #call optimizer.step() to update the model parameters based on the computed gradients\n",
    "        #keep the w1s, w2s, and bs, and loss value some list so you can plot them later\n",
    "\n",
    "#plot the losslist, w1s, w2s, and bs.\n",
    "\n",
    "\n",
    "for epoch in range(10):\n",
    "    np.random.shuffle(D)\n",
    "    for idx in range(len(D)):\n",
    "        optimizer.zero_grad()\n",
    "        x_input = Variable(torch.from_numpy(np.array([X[idx]]))).type(torch.FloatTensor)\n",
    "        y_target = Variable(torch.from_numpy(np.array([Y[idx]]))).type(torch.FloatTensor)\n",
    "        pred_i = model.forward(x_input.view(len(x_input), 2))\n",
    "        lossout = loss(pred_i, y_target.view(len(x_input), 1))\n",
    "        lossout.backward()\n",
    "        optimizer.step()\n",
    "        w1 = model.weight.data.numpy().ravel()[0]\n",
    "        w2 = model.weight.data.numpy().ravel()[1]\n",
    "        b = model.bias.data.numpy().ravel()[0]\n",
    "        w1list.append(w1)\n",
    "        w2list.append(w2)\n",
    "        blist.append(b)\n",
    "        losslist.append(lossout.data.numpy().ravel()[0])\n",
    "\n",
    "plt.plot(w1list)\n",
    "plt.show()\n",
    "plt.plot(w2list)\n",
    "plt.show()\n",
    "plt.plot(blist)\n",
    "plt.show()\n",
    "plt.plot(losslist)\n",
    "plt.show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Question 3: Learning Curves, Overfitting, and Machine Learning! (45 Points + 7 Bonus Points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know how to optimize, let's get some real machine learning done!\n",
    "\n",
    "Instead of the small dataset we had in question 2, now we will use the Diabetes Dataset which can be downloaded from [here](https://drive.google.com/drive/folders/1nuZg4pMFvOZHCHxtU5gBEnHq9YJBdZX_).\n",
    "\n",
    "In this dataset, we are trying to predict if a person doesn't have diabetes (0), has diabetes in Stage 1 or in Stage 2. The output labels can be found in Output.csv. All the other files will be used as Input. From these files, the columns that we are interested in and what they stand for are as follows :\n",
    "                          \n",
    "                          SEQN : ID\n",
    "                          \n",
    "                          RIAGENDR : Gender\n",
    "                          \n",
    "                          DMDYRSUS : Years in US\n",
    "                          \n",
    "                          INDFMPIR : Family income\n",
    "                          \n",
    "                          LBXGH : GlycoHemoglobin\n",
    "                          \n",
    "                          BMXARMC : Arm Circum\n",
    "                          \n",
    "                          BMDAVSAD : Saggital Abdominal\n",
    "                          \n",
    "                          MGDCGSZ : Grip Strength\n",
    "                          \n",
    "                          DRABF : Breast fed\n",
    "\n",
    "We will use the first 6000 samples for training while the rest for testing.\n",
    "\n",
    "Solve questions 3.1 to 3.7 with this information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Data Analysis (5 points)\n",
    "\n",
    "Read the input csvs. Rename the variables with their meaningful names shown above.<br>\n",
    "Print all the desired variables in the training set with their count, mean, standard deviation and range. \n",
    "\n",
    "__Hint__ : If you are using pandas, it might save some time to look for a built-in function that returns all these!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "output=pd.read_csv(\"Output.csv\", encoding='cp1252', index_col= False)\n",
    "new_out_name = [\"ID\", \"Diab_Presence\"]\n",
    "output.columns = new_out_name\n",
    "output = pd.DataFrame(output)\n",
    "\n",
    "labs = pd.read_csv(\"labs.csv\", encoding='cp1252', index_col=False)\n",
    "labs = labs[[\"SEQN\",\"LBXGH\"]]\n",
    "new_labs_name = [\"ID\", \"GlycoHemoglobin\"]\n",
    "labs.columns = new_labs_name\n",
    "labs = pd.DataFrame(labs)\n",
    "\n",
    "exam = pd.read_csv(\"examination.csv\", encoding='cp1252', index_col=False)\n",
    "exam = exam[[\"SEQN\", \"BMXARMC\",\"BMDAVSAD\",\"MGDCGSZ\"]]\n",
    "exam_name = [\"ID\", \"Arm_Circum\", \"Saggital_Abdominal\", \"Grip_Strength\"]\n",
    "exam.columns = exam_name\n",
    "exam = pd.DataFrame(exam)\n",
    "\n",
    "diet = pd.read_csv(\"diet.csv\", encoding='cp1252', index_col=False)\n",
    "diet = diet[[\"SEQN\",\"DRABF\"]]\n",
    "diet_name = [\"ID\", \"Breast_Fed\"]\n",
    "diet.columns = diet_name\n",
    "diet = pd.DataFrame(diet)\n",
    "\n",
    "demo = pd.read_csv(\"demographic.csv\", encoding='cp1252', index_col=False)\n",
    "demo = demo[[\"SEQN\", \"RIAGENDR\",\"DMDYRSUS\",\"INDFMPIR\"]]\n",
    "demo_name = [\"ID\", \"Gender\", \"Years_in_US\", \"Family_Income\"]\n",
    "demo.columns = demo_name\n",
    "demo = pd.DataFrame(demo)\n",
    "\n",
    "diabetes_data = output.merge(labs,on='ID').merge(exam,on='ID').merge(diet,on='ID').merge(demo,on='ID')\n",
    "diabetes_data = pd.DataFrame(diabetes_data)\n",
    "\n",
    "training_set = diabetes_data[:6000]\n",
    "Id = training_set[[\"ID\"]]\n",
    "test_set = diabetes_data.tail(3813)\n",
    "Id_test = test_set[[\"ID\"]]\n",
    "\n",
    "stats = training_set.agg(['count', 'mean', \"std\", \"max\",\"min\"])\n",
    "stats.loc[\"range\"] = stats.loc[\"max\"] - stats.loc[\"min\"]\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Fill in missing values (5 points)\n",
    "\n",
    "Fill in rows for both the train and test sets where the values are missing with the __fillna__ method. Use the following criteria:\n",
    "\n",
    "1. Missing Years in US - 0\n",
    "2. Missing GlycoHemoglobin - median\n",
    "3. Missing Saggital Abdominal - median\n",
    "4. Missing Arm Circum - median\n",
    "5. Missing Grip Strength - median\n",
    "6. Missing Family Income - forward fill\n",
    "7. Missing Breast Fed - 1\n",
    "8. Missing Gender - 2 \n",
    "\n",
    "Median value has to be calculated for the particular column only on the training set.<br>\n",
    "Now create a dataframe with only the desired variables for both train and test sets. Print the training set mean, standard deviation and range again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling the NaN values for missing years in US, missing family income, missing breast fed, missing gender \n",
    "#for both the training set and testing set\n",
    "years_in_us = diabetes_data[[\"Years_in_US\"]].fillna(0)\n",
    "missing_breast_fed = diabetes_data[[\"Breast_Fed\"]].fillna(1)\n",
    "missing_gender = diabetes_data[[\"Gender\"]].fillna(1)\n",
    "missing_family_income = diabetes_data[[\"Family_Income\"]].fillna(method='ffill')\n",
    "\n",
    "diabetes_data = pd.concat([output, years_in_us, missing_gender, missing_family_income, missing_breast_fed, labs, exam], axis=1)\n",
    "\n",
    "#filling the NaN values for missing GlycoHemoglobin, missing Saggital Abdominal, missing Arm Circum, missing Grip Strength\n",
    "#for training set only\n",
    "train_set = diabetes_data[:6000]\n",
    "\n",
    "missing_glyco = train_set[[\"GlycoHemoglobin\"]].fillna(train_set[[\"GlycoHemoglobin\"]].median())\n",
    "missing_sag_abs = train_set[[\"Saggital_Abdominal\"]].fillna(train_set[[\"Saggital_Abdominal\"]].median())\n",
    "missing_arm_circ = train_set[[\"Arm_Circum\"]].fillna(train_set[[\"Arm_Circum\"]].median())\n",
    "missing_grip_str = train_set[[\"Grip_Strength\"]].fillna(train_set[[\"Grip_Strength\"]].median())\n",
    "\n",
    "combine = train_set[[\"Diab_Presence\",\"Years_in_US\", \"Gender\", \"Family_Income\", \"Breast_Fed\"]]\n",
    "train_set = pd.concat([Id, combine, missing_glyco, missing_sag_abs, missing_arm_circ, missing_grip_str], axis=1)\n",
    "train_set[\"ID\"] = pd.to_numeric(train_set[\"ID\"])\n",
    "\n",
    "#filling the NaN values for missing GlycoHemoglobin, missing Saggital Abdominal, missing Arm Circum, missing Grip Strength\n",
    "#for testing set only\n",
    "test_set = diabetes_data.tail(3813)\n",
    "\n",
    "missing_glyco_ts= test_set[[\"GlycoHemoglobin\"]].fillna(train_set[[\"GlycoHemoglobin\"]].median())\n",
    "missing_sag_abs_ts = test_set[[\"Saggital_Abdominal\"]].fillna(train_set[[\"Saggital_Abdominal\"]].median())\n",
    "missing_arm_circ_ts = test_set[[\"Arm_Circum\"]].fillna(train_set[[\"Arm_Circum\"]].median())\n",
    "missing_grip_str_ts = test_set[[\"Grip_Strength\"]].fillna(train_set[[\"Grip_Strength\"]].median())\n",
    "\n",
    "combine_ts = test_set[[\"Diab_Presence\",\"Years_in_US\", \"Gender\", \"Family_Income\", \"Breast_Fed\"]]\n",
    "test_set = pd.concat([Id_test, combine_ts, missing_glyco_ts, missing_sag_abs_ts, missing_arm_circ_ts, missing_grip_str_ts], axis=1)\n",
    "test_set[\"ID\"] = pd.to_numeric(test_set[\"ID\"])\n",
    "\n",
    "#printing the mean, standard deviation, and range for TRAINING SET (with no NaN) only\n",
    "stats = train_set.agg(['count', 'mean', \"std\", \"max\",\"min\"])\n",
    "stats.loc[\"range\"] = stats.loc[\"max\"] - stats.loc[\"min\"]\n",
    "stats\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 The DataLoader (5 points)\n",
    "\n",
    "Write an error-free dataloader class in Pytorch for our dataset. \n",
    "\n",
    "If you need help in writing a dataloader class, read more about it __[here](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class diabetes_train_set(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        xy = np.loadtxt(\"train_set.csv\", delimiter=',', dtype=np.float32)\n",
    "        self.len = xy.shape[0]\n",
    "        self.x_data = torch.from_numpy(xy[:, 0:-1])\n",
    "        self.y_data = torch.from_numpy(xy[:, [-1]])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "dataset = diabetes_train_set()\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=200, shuffle=True, num_workers=2)\n",
    "\n",
    "class diabetes_test_set(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        xy = np.loadtxt(\"test_set.csv\", delimiter=',', dtype=np.float32)\n",
    "        self.len = xy.shape[0]\n",
    "        self.x_data = torch.from_numpy(xy[:, 0:-1])\n",
    "        self.y_data = torch.from_numpy(xy[:, [-1]])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "dataset = diabetes_test_set()\n",
    "test_loader = DataLoader(dataset=dataset, batch_size=200, shuffle=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 The Network (5 points + 2 Bonus points)\n",
    "\n",
    "Write an error-free 2 hidden layer Multilayer Perceptron class using Pytorch. Use ReLU as the non-linearity. The hidden layer sizes are 1000-300.\n",
    "\n",
    "__Bonus__: Write a function to initialize the layer weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class DeepNet(nn.Module):\n",
    "    def __init__(self, in_features, hidden_size_1=1000, hidden_size_2=300, out_features=3):\n",
    "        super(DeepNet, self).__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(in_features, hidden_size_1),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(hidden_size_1, hidden_size_2),\n",
    "                                 nn.ReLU(),\n",
    "                            nn.Linear(hidden_size_2, out_features))\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out\n",
    "DeepNet(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 The Training phase (10 points)\n",
    "\n",
    "Write a function to train your model and the rest of the script required to run this.<br>\n",
    "Plot the average loss per epoch for 100 epochs. Report the loss values at epoch 100.<br>\n",
    "Parameters to be used are as follows:<br>\n",
    "\n",
    "Optimizer : Stochastic Gradient Descent<br>\n",
    "Learning Rate : 1e-4<br>\n",
    "Loss : Cross Entropy<br>\n",
    "Batchsize : 200<br>\n",
    "Shuffle : True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "model = DeepNet(8)\n",
    "model = model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#batch size of 200 is indicated in the parameters of the dataloader.\n",
    "loss_list = []\n",
    "for epoch in range(100):  # loop over the dataset for 100 epochs\n",
    "    avg_loss = 0\n",
    "    for i, data in enumerate(train_loader, 0):  # trainloader reads data using torchvision\n",
    "        inputs, labels = data\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        avg_loss += loss.data.numpy().ravel()[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss_list.append(avg_loss/inputs.shape[0])\n",
    "plt.plot(loss_list)\n",
    "print(\"The loss value at 100th epoch:\", loss_list[99])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 The Testing Phase (10 points)\n",
    "\n",
    "Write a function to test your trained model.<br>Report the average test loss and the testing accuracy.<br>Also print the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for data in test_loader:\n",
    "    inputs, labels = data\n",
    "    outputs = model(Variable(inputs))\n",
    "    _, predicted = torch.max(outputs.data, 1)   # Find the class index with the maximum value.\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "model = DeepNet(8)\n",
    "model = mode.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#batch size of 200 is indicated in the parameters of the dataloader.\n",
    "loss_list_train = []\n",
    "loss_list_test = []\n",
    "for epoch in range(100):  # loop over the dataset for 100 epochs\n",
    "    avg_loss = 0\n",
    "    for i, data in enumerate(train_loader, 0):  # trainloader reads data using torchvision\n",
    "        inputs, labels = data\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        avg_loss += loss.data.numpy().ravel()[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss_list_train.append(avg_loss/inputs.shape[0])\n",
    "    \n",
    "    for i, data in enumerate(test_loader, 0):  # trainloader reads data using torchvision\n",
    "        inputs, labels = data\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        avg_loss += loss.data.numpy().ravel()[0]\n",
    "        loss_list_test.append(avg_loss/inputs.shape[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss_list_test.append(avg_loss/inputs.shape[0])\n",
    "    confusion_matrix(outputs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 The AUCs (5 points + 5 Bonus points)\n",
    "Now calculate the AUC of your trained model on the test set.<br>\n",
    "**(Bonus)** Use the sklearn Multiclass Logistic Regression model and fit it to our dataset. Calculate the AUC of this model. Compare the two AUCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
